{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started scraping info for: karinalarrauri\n",
      "Done scraping info for: karinalarrauri\n",
      "Account info appended for karinalarrauri\n",
      "Started scraping info for: karina_nigay\n",
      "Done scraping info for: karina_nigay\n",
      "Account info appended for karina_nigay\n",
      "Started scraping info for: kateclapp\n",
      "Done scraping info for: kateclapp\n",
      "Account info appended for kateclapp\n",
      "Started scraping info for: katerina_perez\n",
      "Done scraping info for: katerina_perez\n",
      "Account info appended for katerina_perez\n",
      "Started scraping info for: katia_mi\n",
      "Failed to fetch or append info for katia_mi: 'edge_followed_by'\n",
      "Started scraping info for: katia_mi_\n",
      "Done scraping info for: katia_mi_\n",
      "Account info appended for katia_mi_\n",
      "Started scraping info for: katya_mtsitouridze\n",
      "Done scraping info for: katya_mtsitouridze\n",
      "Account info appended for katya_mtsitouridze\n",
      "Started scraping info for: kat_in_nyc\n",
      "Done scraping info for: kat_in_nyc\n",
      "Account info appended for kat_in_nyc\n",
      "Started scraping info for: kayla_itsines\n",
      "Done scraping info for: kayla_itsines\n",
      "Account info appended for kayla_itsines\n",
      "Started scraping info for: keenanpearce\n",
      "Done scraping info for: keenanpearce\n",
      "Account info appended for keenanpearce\n",
      "Started scraping info for: keiyamazaki\n",
      "Done scraping info for: keiyamazaki\n",
      "Account info appended for keiyamazaki\n",
      "Started scraping info for: kelsbrianne\n",
      "Failed to fetch or append info for kelsbrianne: 'edge_followed_by'\n",
      "Started scraping info for: kelsrfloyd\n",
      "Done scraping info for: kelsrfloyd\n",
      "Account info appended for kelsrfloyd\n",
      "Started scraping info for: kenzas\n",
      "Done scraping info for: kenzas\n",
      "Account info appended for kenzas\n",
      "Started scraping info for: kerrently\n",
      "Done scraping info for: kerrently\n",
      "Account info appended for kerrently\n",
      "Started scraping info for: kerriehessillustration\n",
      "Done scraping info for: kerriehessillustration\n",
      "Account info appended for kerriehessillustration\n",
      "Started scraping info for: kessara\n",
      "Done scraping info for: kessara\n",
      "Account info appended for kessara\n",
      "Started scraping info for: kewiki\n",
      "Done scraping info for: kewiki\n",
      "Account info appended for kewiki\n",
      "Started scraping info for: khivju\n",
      "Done scraping info for: khivju\n",
      "Account info appended for khivju\n",
      "Started scraping info for: kickposters\n",
      "Done scraping info for: kickposters\n",
      "Account info appended for kickposters\n",
      "Started scraping info for: kilianjornet\n",
      "Done scraping info for: kilianjornet\n",
      "Account info appended for kilianjornet\n",
      "Started scraping info for: kinoyoga\n",
      "Done scraping info for: kinoyoga\n",
      "Account info appended for kinoyoga\n",
      "Started scraping info for: kirstenalana\n",
      "Done scraping info for: kirstenalana\n",
      "Account info appended for kirstenalana\n",
      "Started scraping info for: kirstenzellers\n",
      "Done scraping info for: kirstenzellers\n",
      "Account info appended for kirstenzellers\n",
      "Started scraping info for: kitkat_ch\n",
      "Done scraping info for: kitkat_ch\n",
      "Account info appended for kitkat_ch\n",
      "Started scraping info for: knifeaxe\n",
      "Failed to fetch or append info for knifeaxe: 'edge_followed_by'\n",
      "Started scraping info for: kochifaraj\n",
      "Done scraping info for: kochifaraj\n",
      "Account info appended for kochifaraj\n",
      "Started scraping info for: konaction\n",
      "Done scraping info for: konaction\n",
      "Account info appended for konaction\n",
      "Started scraping info for: konservatif\n",
      "Done scraping info for: konservatif\n",
      "Account info appended for konservatif\n",
      "Started scraping info for: kosta_williams\n",
      "Done scraping info for: kosta_williams\n",
      "Account info appended for kosta_williams\n",
      "Started scraping info for: kpunkka\n",
      "Done scraping info for: kpunkka\n",
      "Account info appended for kpunkka\n",
      "Started scraping info for: kristinabazan\n",
      "Done scraping info for: kristinabazan\n",
      "Account info appended for kristinabazan\n",
      "Started scraping info for: kthegroove\n",
      "Done scraping info for: kthegroove\n",
      "Account info appended for kthegroove\n",
      "Started scraping info for: kyrenian\n",
      "Done scraping info for: kyrenian\n",
      "Account info appended for kyrenian\n",
      "Started scraping info for: ladyaddict\n",
      "Done scraping info for: ladyaddict\n",
      "Account info appended for ladyaddict\n",
      "Started scraping info for: ladzinski\n",
      "Done scraping info for: ladzinski\n",
      "Account info appended for ladzinski\n",
      "Started scraping info for: landonnordeman\n",
      "Done scraping info for: landonnordeman\n",
      "Account info appended for landonnordeman\n",
      "Started scraping info for: lane_toran\n",
      "Done scraping info for: lane_toran\n",
      "Account info appended for lane_toran\n",
      "Started scraping info for: lasselom\n",
      "Done scraping info for: lasselom\n",
      "Account info appended for lasselom\n",
      "Started scraping info for: latifitness\n",
      "Done scraping info for: latifitness\n",
      "Account info appended for latifitness\n",
      "Started scraping info for: laurajenkinson\n",
      "Failed to fetch or append info for laurajenkinson: 'edge_followed_by'\n",
      "Started scraping info for: laurasykora\n",
      "Failed to fetch or append info for laurasykora: 'edge_followed_by'\n",
      "Started scraping info for: laurenepbath\n",
      "Done scraping info for: laurenepbath\n",
      "Account info appended for laurenepbath\n",
      "Started scraping info for: laurensimpson\n",
      "Done scraping info for: laurensimpson\n",
      "Account info appended for laurensimpson\n",
      "Started scraping info for: lavicvic\n",
      "Done scraping info for: lavicvic\n",
      "Account info appended for lavicvic\n",
      "Started scraping info for: leagueofupdates\n",
      "Failed to fetch or append info for leagueofupdates: 'edge_followed_by'\n",
      "Started scraping info for: leighannsays\n",
      "Done scraping info for: leighannsays\n",
      "Account info appended for leighannsays\n",
      "Started scraping info for: lennart\n",
      "Done scraping info for: lennart\n",
      "Account info appended for lennart\n",
      "Started scraping info for: levijfoster\n",
      "Done scraping info for: levijfoster\n",
      "Account info appended for levijfoster\n",
      "Started scraping info for: le_blanc\n",
      "Done scraping info for: le_blanc\n",
      "Account info appended for le_blanc\n",
      "Started scraping info for: libesona\n",
      "Done scraping info for: libesona\n",
      "Account info appended for libesona\n",
      "Started scraping info for: lilithmoonlife\n",
      "Failed to fetch or append info for lilithmoonlife: 'edge_followed_by'\n",
      "Started scraping info for: lilymaymac\n",
      "Done scraping info for: lilymaymac\n",
      "Account info appended for lilymaymac\n",
      "Started scraping info for: linatesch\n",
      "Done scraping info for: linatesch\n",
      "Account info appended for linatesch\n",
      "Started scraping info for: linda_lomelino\n",
      "Done scraping info for: linda_lomelino\n",
      "Account info appended for linda_lomelino\n",
      "Started scraping info for: lioninthewild\n",
      "Done scraping info for: lioninthewild\n",
      "Account info appended for lioninthewild\n",
      "Started scraping info for: lipstickfables\n",
      "Done scraping info for: lipstickfables\n",
      "Account info appended for lipstickfables\n",
      "Started scraping info for: lisahyde_\n",
      "Done scraping info for: lisahyde_\n",
      "Account info appended for lisahyde_\n",
      "Started scraping info for: littlelizziev\n",
      "Done scraping info for: littlelizziev\n",
      "Account info appended for littlelizziev\n",
      "Started scraping info for: livemontreal\n",
      "Done scraping info for: livemontreal\n",
      "Account info appended for livemontreal\n",
      "Started scraping info for: livingitrural\n",
      "Done scraping info for: livingitrural\n",
      "Account info appended for livingitrural\n",
      "Started scraping info for: livingonearth\n",
      "Done scraping info for: livingonearth\n",
      "Account info appended for livingonearth\n",
      "Started scraping info for: lizaonair\n",
      "Done scraping info for: lizaonair\n",
      "Account info appended for lizaonair\n",
      "Started scraping info for: local_milk\n",
      "Done scraping info for: local_milk\n",
      "Account info appended for local_milk\n",
      "Started scraping info for: lojain_omran\n",
      "Done scraping info for: lojain_omran\n",
      "Account info appended for lojain_omran\n",
      "Started scraping info for: louisdarcis\n",
      "Done scraping info for: louisdarcis\n",
      "Account info appended for louisdarcis\n",
      "Started scraping info for: louteasdale\n",
      "Done scraping info for: louteasdale\n",
      "Account info appended for louteasdale\n",
      "Started scraping info for: loveandlemons\n",
      "Done scraping info for: loveandlemons\n",
      "Account info appended for loveandlemons\n",
      "Started scraping info for: lovelypepa\n",
      "Done scraping info for: lovelypepa\n",
      "Account info appended for lovelypepa\n",
      "Started scraping info for: luanasilva\n",
      "Done scraping info for: luanasilva\n",
      "Account info appended for luanasilva\n",
      "Started scraping info for: lucamacellaripalmieri\n",
      "Done scraping info for: lucamacellaripalmieri\n",
      "Account info appended for lucamacellaripalmieri\n",
      "Started scraping info for: luckybsmith\n",
      "Done scraping info for: luckybsmith\n",
      "Account info appended for luckybsmith\n",
      "Started scraping info for: lumadeline\n",
      "Done scraping info for: lumadeline\n",
      "Account info appended for lumadeline\n",
      "Started scraping info for: luxurybridalblogger\n",
      "Done scraping info for: luxurybridalblogger\n",
      "Account info appended for luxurybridalblogger\n",
      "Started scraping info for: lyzabethlopez\n",
      "Done scraping info for: lyzabethlopez\n",
      "Account info appended for lyzabethlopez\n",
      "Started scraping info for: maaren_xx\n",
      "Done scraping info for: maaren_xx\n",
      "Account info appended for maaren_xx\n",
      "Started scraping info for: macenzo\n",
      "Done scraping info for: macenzo\n",
      "Account info appended for macenzo\n",
      "Started scraping info for: madds\n",
      "Done scraping info for: madds\n",
      "Account info appended for madds\n",
      "Started scraping info for: madwhips_bimmer\n",
      "Done scraping info for: madwhips_bimmer\n",
      "Account info appended for madwhips_bimmer\n",
      "Started scraping info for: m_eye_nd\n",
      "Done scraping info for: m_eye_nd\n",
      "Account info appended for m_eye_nd\n"
     ]
    }
   ],
   "source": [
    "import http.client\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "\n",
    "def fetch_account_info(conn, username, headers):\n",
    "    conn.request(\"GET\", \"/account-info?username=\" + username, headers=headers)\n",
    "\n",
    "    res = conn.getresponse()\n",
    "    data = res.read()\n",
    "    json_string = data.decode('utf-8')\n",
    "    parsed_data = json.loads(json_string)\n",
    "    return parsed_data\n",
    "\n",
    "def scrape_account_info(username):\n",
    "    print(f\"Started scraping info for: {username}\")\n",
    "    conn = http.client.HTTPSConnection(\"instagram130.p.rapidapi.com\")\n",
    "\n",
    "    headers = {\n",
    "        'X-RapidAPI-Key': \"30c2cf4e50msh31cbf1006cff3adp1652e3jsn64f9f56d7cb1\",\n",
    "        'X-RapidAPI-Host': \"instagram130.p.rapidapi.com\"\n",
    "    }\n",
    "    data = fetch_account_info(conn, username, headers)\n",
    "    account_info = {\n",
    "        'username': username,\n",
    "        'followers': data['edge_followed_by']['count'],\n",
    "        'following': data['edge_follow']['count']\n",
    "    }\n",
    "    print(f\"Done scraping info for: {username}\")\n",
    "    return account_info\n",
    "\n",
    "def append_to_csv(data, filename):\n",
    "    file_exists = os.path.isfile(filename)\n",
    "\n",
    "    with open(filename, 'a', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=data.keys())\n",
    "\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "\n",
    "        writer.writerow(data)\n",
    "\n",
    "def scrape_and_append(username, filename):\n",
    "    try:\n",
    "        account_info = scrape_account_info(username)\n",
    "        append_to_csv(account_info, filename)\n",
    "        print(f\"Account info appended for {username}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch or append info for {username}: {e}\")\n",
    "\n",
    "accounts_list = ['coryrichards', 'couturenotebook', 'cozinhadalbo', 'cravingsinamsterdam', 'crazyjewishmom', 'cristinamittermeier', 'crochetgirl99', 'croyable', 'cuchira', 'cucinadigitale', 'culturizate', 'cutegirlshairstyles', 'cyarine', 'd.signers', 'daddyissues_', 'dailyfoodfeed', 'daisyclementine', 'dalalid', 'damselindior', 'danbilzerian', 'dani_thorne', 'daniel_ernst', 'dannibelle', 'dannyzappa', 'danrubin', 'dansmoe', 'daveyoder', 'davidalanharvey', 'daviddoubilet', 'davidson_frere', 'davidstea', 'dawilda', 'deemabayyaa', 'deliciouslyella', 'deliscake', 'deporloversteam', 'der_landgraf', 'designdecor', 'destination_wow', 'destinationwolf', 'devanondeck', 'devourpower', 'dguttenfelder', 'diegobarrueco', 'digernes', 'diipakhosla', 'diogopicarra', 'disney_nuts', 'doctor.mike', 'doentesporfutebol', 'doina', 'donkarlito_', 'donnahaymagazine', 'dotzsoh', 'doyoutravel', 'drewtrush', 'drmusatokmak', 'dulceida', 'dunk', 'eatandshout', 'eatfamous', 'eatingnyc', 'eddypinto_', 'edkashi', 'edmhumor', 'el_kilombo', 'elaine_yiu', 'elenacarriere.official', 'elensham', 'elisabeth.rioux', 'elizabethgadd', 'eljuanpazurita', 'elladvornik', 'elliott.mae', 'elliottsailors', 'elsas_wholesomelife', 'emily_katz', 'emilyostbro', 'emilyschuman', 'emilyskyefit', 'emwng', 'enjoyphoenix', 'epictravelpost', 'erikjonesart', 'ernandaputra', 'erubes1', 'escolhiesperar', 'estonianna', 'eternal_noir', 'europe.vacations', 'euvictornogueira', 'evakosmasflores', 'everlook_photography', 'everythingeverywhere', 'exoticsbrazil', 'extremenature', 'f1mike28', 'faafirds', 'fabumakeup4u', 'faby_mamaedegemeos', 'faheeym', 'faktastisch', 'fantastic_earth', 'fashion_by_gi', 'fashionboystyle', 'fashionforfemmes', 'fatosfemininos', 'featureshoot', 'felicitegrace', 'felixsiauw', 'filippofiora', 'finn', 'fit_trio', 'fitopaezmusica', 'fitqueenirene', 'fixthisbuildthat', 'flamuruk', 'flettemamma', 'fluffypack', 'forn_sant_francesc', 'fozaza', 'framboisejam', 'francescocostagliola_', 'frangocombatatadoce', 'frank_the_funnyfrenchie', 'franslanting', 'frasesdem3rda', 'frasibianche.it', 'frenchiebutt', 'frenchwords', 'funforlouis', 'gabrielapugliesi', 'gabyespino', 'galagonzalez', 'garancedore', 'garypeppergirl', 'gatherandfeast', 'gavinoneillphoto', 'gentbelike', 'gentslounge', 'geosteinmetz', 'gerdludwig', 'germanroamers', 'giangi_81', 'gianmarcovalenza', 'giizeleoliveira', 'ginizzle', 'giulianafortuna', 'gizemozdem1', 'gkstories', 'glencyfeliz', 'globe.travelpix', 'goldengianpy', 'goldenretrievers_', 'goodnewsfeed', 'gregsideris', 'grungesource', 'gui_', 'gurukafa', 'guy_tang', 'guytang_mydentity', 'gwilymcpugh', 'gypsea_lust', 'gypsetgoddess', 'hadiaghaleb', 'hairgod_zito']\n",
    "\n",
    "csv_filename = \"./trisha/accountinfo/accounts_info.csv\"\n",
    "\n",
    "for account in accounts_list:\n",
    "    time.sleep(2)  \n",
    "    scrape_and_append(account, csv_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
