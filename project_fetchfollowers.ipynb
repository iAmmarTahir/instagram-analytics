{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started scraping info for: christendominique\n",
      "Done scraping info for: christendominique\n",
      "Account info appended for christendominique\n",
      "Started scraping info for: christianziegler\n",
      "Done scraping info for: christianziegler\n",
      "Account info appended for christianziegler\n",
      "Started scraping info for: christinabiluca\n",
      "Failed to fetch or append info for christinabiluca: 'edge_followed_by'\n",
      "Started scraping info for: christinehmcconnell\n",
      "Done scraping info for: christinehmcconnell\n",
      "Account info appended for christinehmcconnell\n",
      "Started scraping info for: christopherandersonphoto\n",
      "Done scraping info for: christopherandersonphoto\n",
      "Account info appended for christopherandersonphoto\n",
      "Started scraping info for: chuck\n",
      "Done scraping info for: chuck\n",
      "Account info appended for chuck\n",
      "Started scraping info for: cimkedi\n",
      "Done scraping info for: cimkedi\n",
      "Account info appended for cimkedi\n",
      "Started scraping info for: cintascotch\n",
      "Done scraping info for: cintascotch\n",
      "Account info appended for cintascotch\n",
      "Started scraping info for: ciriljazbec\n",
      "Done scraping info for: ciriljazbec\n",
      "Account info appended for ciriljazbec\n",
      "Started scraping info for: civilking\n",
      "Done scraping info for: civilking\n",
      "Account info appended for civilking\n",
      "Started scraping info for: claireonline\n",
      "Done scraping info for: claireonline\n",
      "Account info appended for claireonline\n",
      "Started scraping info for: clarecrawley\n",
      "Done scraping info for: clarecrawley\n",
      "Account info appended for clarecrawley\n",
      "Started scraping info for: classiccarchasers\n",
      "Done scraping info for: classiccarchasers\n",
      "Account info appended for classiccarchasers\n",
      "Started scraping info for: colbybrownphotography\n",
      "Done scraping info for: colbybrownphotography\n",
      "Account info appended for colbybrownphotography\n",
      "Started scraping info for: colerise\n",
      "Done scraping info for: colerise\n",
      "Account info appended for colerise\n",
      "Started scraping info for: collagevintage\n",
      "Done scraping info for: collagevintage\n",
      "Account info appended for collagevintage\n",
      "Started scraping info for: college_housewife\n",
      "Done scraping info for: college_housewife\n",
      "Account info appended for college_housewife\n",
      "Started scraping info for: conrad_anker\n",
      "Done scraping info for: conrad_anker\n",
      "Account info appended for conrad_anker\n",
      "Started scraping info for: coryrichards\n"
     ]
    }
   ],
   "source": [
    "import http.client\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "\n",
    "def fetch_account_info(conn, username, headers):\n",
    "    conn.request(\"GET\", \"/account-info?username=\" + username, headers=headers)\n",
    "\n",
    "    res = conn.getresponse()\n",
    "    data = res.read()\n",
    "    json_string = data.decode('utf-8')\n",
    "    parsed_data = json.loads(json_string)\n",
    "    return parsed_data\n",
    "\n",
    "def scrape_account_info(username):\n",
    "    print(f\"Started scraping info for: {username}\")\n",
    "    conn = http.client.HTTPSConnection(\"instagram130.p.rapidapi.com\")\n",
    "\n",
    "    headers = {\n",
    "        'X-RapidAPI-Key': \"30c2cf4e50msh31cbf1006cff3adp1652e3jsn64f9f56d7cb1\",\n",
    "        'X-RapidAPI-Host': \"instagram130.p.rapidapi.com\"\n",
    "    }\n",
    "    data = fetch_account_info(conn, username, headers)\n",
    "    account_info = {\n",
    "        'username': username,\n",
    "        'followers': data['edge_followed_by']['count'],\n",
    "        'following': data['edge_follow']['count']\n",
    "    }\n",
    "    print(f\"Done scraping info for: {username}\")\n",
    "    return account_info\n",
    "\n",
    "def append_to_csv(data, filename):\n",
    "    file_exists = os.path.isfile(filename)\n",
    "\n",
    "    with open(filename, 'a', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=data.keys())\n",
    "\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "\n",
    "        writer.writerow(data)\n",
    "\n",
    "def scrape_and_append(username, filename):\n",
    "    try:\n",
    "        account_info = scrape_account_info(username)\n",
    "        append_to_csv(account_info, filename)\n",
    "        print(f\"Account info appended for {username}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch or append info for {username}: {e}\")\n",
    "\n",
    "accounts_list = ['coryrichards', 'couturenotebook', 'cozinhadalbo', 'cravingsinamsterdam', 'crazyjewishmom', 'cristinamittermeier', 'crochetgirl99', 'croyable', 'cuchira', 'cucinadigitale', 'culturizate', 'cutegirlshairstyles', 'cyarine', 'd.signers', 'daddyissues_', 'dailyfoodfeed', 'daisyclementine', 'dalalid', 'damselindior', 'danbilzerian', 'dani_thorne', 'daniel_ernst', 'dannibelle', 'dannyzappa', 'danrubin', 'dansmoe', 'daveyoder', 'davidalanharvey', 'daviddoubilet', 'davidson_frere', 'davidstea', 'dawilda', 'deemabayyaa', 'deliciouslyella', 'deliscake', 'deporloversteam', 'der_landgraf', 'designdecor', 'destination_wow', 'destinationwolf', 'devanondeck', 'devourpower', 'dguttenfelder', 'diegobarrueco', 'digernes', 'diipakhosla', 'diogopicarra', 'disney_nuts', 'doctor.mike', 'doentesporfutebol', 'doina', 'donkarlito_', 'donnahaymagazine', 'dotzsoh', 'doyoutravel', 'drewtrush', 'drmusatokmak', 'dulceida', 'dunk', 'eatandshout', 'eatfamous', 'eatingnyc', 'eddypinto_', 'edkashi', 'edmhumor', 'el_kilombo', 'elaine_yiu', 'elenacarriere.official', 'elensham', 'elisabeth.rioux', 'elizabethgadd', 'eljuanpazurita', 'elladvornik', 'elliott.mae', 'elliottsailors', 'elsas_wholesomelife', 'emily_katz', 'emilyostbro', 'emilyschuman', 'emilyskyefit', 'emwng', 'enjoyphoenix', 'epictravelpost', 'erikjonesart', 'ernandaputra', 'erubes1', 'escolhiesperar', 'estonianna', 'eternal_noir', 'europe.vacations', 'euvictornogueira', 'evakosmasflores', 'everlook_photography', 'everythingeverywhere', 'exoticsbrazil', 'extremenature', 'f1mike28', 'faafirds', 'fabumakeup4u', 'faby_mamaedegemeos', 'faheeym', 'faktastisch', 'fantastic_earth', 'fashion_by_gi', 'fashionboystyle', 'fashionforfemmes', 'fatosfemininos', 'featureshoot', 'felicitegrace', 'felixsiauw', 'filippofiora', 'finn', 'fit_trio', 'fitopaezmusica', 'fitqueenirene', 'fixthisbuildthat', 'flamuruk', 'flettemamma', 'fluffypack', 'forn_sant_francesc', 'fozaza', 'framboisejam', 'francescocostagliola_', 'frangocombatatadoce', 'frank_the_funnyfrenchie', 'franslanting', 'frasesdem3rda', 'frasibianche.it', 'frenchiebutt', 'frenchwords', 'funforlouis', 'gabrielapugliesi', 'gabyespino', 'galagonzalez', 'garancedore', 'garypeppergirl', 'gatherandfeast', 'gavinoneillphoto', 'gentbelike', 'gentslounge', 'geosteinmetz', 'gerdludwig', 'germanroamers', 'giangi_81', 'gianmarcovalenza', 'giizeleoliveira', 'ginizzle', 'giulianafortuna', 'gizemozdem1', 'gkstories', 'glencyfeliz', 'globe.travelpix', 'goldengianpy', 'goldenretrievers_', 'goodnewsfeed', 'gregsideris', 'grungesource', 'gui_', 'gurukafa', 'guy_tang', 'guytang_mydentity', 'gwilymcpugh', 'gypsea_lust', 'gypsetgoddess', 'hadiaghaleb', 'hairgod_zito']\n",
    "\n",
    "csv_filename = \"./trisha/accountinfo/accounts_info.csv\"\n",
    "\n",
    "for account in accounts_list:\n",
    "    time.sleep(2)  \n",
    "    scrape_and_append(account, csv_filename)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
